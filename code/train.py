import argparse
import random
import warnings
from datetime import datetime

import numpy as np
import pandas as pd
import pytz
import torch
import transformers
import yaml
from heatmap import save_difference_png
from load_data import *
from metrics import *
from custom_robertamodel import CustomRobertaForSequenceClassification
from pyprnt import prnt
from transformers import (
    AutoConfig,
    AutoModelForSequenceClassification,
    AutoTokenizer,
    EarlyStoppingCallback,
    Trainer,
    TrainerCallback,
    TrainingArguments,
)

import wandb


def set_seed(seed: int):
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    np.random.seed(seed)
    random.seed(seed)


def warning_block() -> None:
    # ê²½ê³  ì œê±°, í•¨ìˆ˜ì˜ ë°˜í™˜ì€ ì—†ìŠµë‹ˆë‹¤.
    transformers.logging.set_verbosity_error()
    warnings.filterwarnings("ignore", ".*Some weights of*")
    warnings.filterwarnings("ignore", ".*- This IS*")
    warnings.filterwarnings("ignore", ".*You should probably*")


def save_difference(preds, micro_f1, auprc):
    # valid datasetì— ëŒ€í•œ predictê°’ê³¼ ì‹¤ì œ ë¼ë²¨ê°’ì„ ë¹„êµí•´ì„œ ì˜¤ë‹µíŒŒì¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
    difference = pd.read_csv(cfg["path"]["valid_path"])  # ê¸°ì¡´ valid_dataset ë¶ˆëŸ¬ì™€ì„œ sourceì—´ ì‚­ì œ
    difference = difference.drop(columns=["source"])
    with open(cfg["path"]["dict_num_to_label"], "rb") as f:  # ì˜ˆì¸¡í•œ numberí˜•íƒœì˜ label ê°’ì„ label ì› ìƒíƒœë¡œ ë³µêµ¬
        dict_num_to_label = pickle.load(f)
    labels = [dict_num_to_label[s] for s in preds]
    difference["predict"] = labels
    condition = difference["predict"] == difference["label"]  # ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì´ ê°™ì€ ê²ƒì€ ìœ„ì— ì •ë ¬í•˜ê¸° ìœ„í•œ ì½”ë“œ
    difference["wrong"] = 1
    difference.loc[condition, "wrong"] = 0  # í‹€ë¦¬ë©´ 1, ë§ìœ¼ë©´ 0
    difference_sorted = pd.concat([difference[~condition], difference[condition]])

    MODEL_NAME = cfg["params"]["MODEL_NAME"]  # csv ì´ë¦„ ì„¤ì •
    if "/" in MODEL_NAME:
        MODEL_NAME = MODEL_NAME.split("/")[0] + "-" + MODEL_NAME.split("/")[-1]

    difference_sorted.to_csv(
        cfg["path"]["difference_path"]
        + "difference_"
        + MODEL_NAME
        + "_"
        + str(cfg["params"]["num_train_epochs"])
        + "_"
        + str(cfg["params"]["per_device_train_batch_size"])
        + "_f1_"
        + str(round(micro_f1, 2))
        + "_auprc_"
        + str(round(auprc, 2))
        + ".csv",
        index=False,
    )


class EarlyStoppingCallback(TrainerCallback):
    def __init__(self, early_stopping_patience, early_stopping_threshold, early_stopping_metric, early_stopping_metric_minimize):
        self.early_stopping_patience = early_stopping_patience
        self.early_stopping_threshold = early_stopping_threshold
        self.early_stopping_metric = early_stopping_metric
        self.early_stopping_metric_minimize = early_stopping_metric_minimize
        self.best_metric = float("inf") if self.early_stopping_metric_minimize else float("-inf")
        self.waiting_steps = 0

    def on_log(self, args, state, control, logs=None, model=None, **kwargs):
        current_metric = logs.get(self.early_stopping_metric, None)
        if current_metric is not None:
            if (self.early_stopping_metric_minimize and current_metric < self.best_metric) or (not self.early_stopping_metric_minimize and current_metric > self.best_metric):
                self.best_metric = current_metric
                self.waiting_steps = 0
            else:
                self.waiting_steps += 1

                if self.waiting_steps >= self.early_stopping_patience:
                    print(f"Early stopping triggered after {self.waiting_steps} steps without improvement.")
                    control.should_training_stop = True


def train():
    warning_block()

    seed = cfg["params"]["seeds"]
    set_seed(seed)  # ëœë¤ì‹œë“œ ì„¸íŒ… í•¨ìˆ˜

    # load model and tokenizer
    MODEL_NAME = cfg["params"]["MODEL_NAME"]
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

    info = {
        "Model Name": MODEL_NAME,
        "Train Data": cfg["path"]["train_path"].split("/")[-1],
        "Valid Data": cfg["path"]["valid_path"].split("/")[-1],
        "Epoch": cfg["params"]["num_train_epochs"],
        "Learning Rate": cfg["params"]["learning_rate"],
        "Batch Size": cfg["params"]["per_device_train_batch_size"],
    }

    wandb.init(
        config=cfg,
        entity="hello-jobits",
        project="<Lv2-KLUE>",
        name=f"{MODEL_NAME}_{cfg['params']['num_train_epochs']:02f}_{cfg['params']['per_device_train_batch_size']}_{cfg['params']['learning_rate']}_{datetime.now(pytz.timezone('Asia/Seoul')):%y%m%d%H%M}",
    )
    # wandb ì—ì„œ ì´ ëª¨ë¸ì— ì–´ë–¤ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ê°€ ì‚¬ìš©ë˜ì—ˆëŠ”ì§€ ì €ì¥í•˜ê¸° ìœ„í•´, cfg íŒŒì¼ë¡œ ì„¤ì •ì„ ë¡œê¹…í•©ë‹ˆë‹¤.
    wandb.config.update(cfg)

    # Trainer Callback ìƒì„±
    early_stopping_callback = EarlyStoppingCallback(
        early_stopping_patience=cfg["params"]["early_stopping_patience"],  # ì¡°ê¸° ì¤‘ì§€ê¹Œì§€ì˜ ê¸°ë‹¤ë¦¬ëŠ” íšŸìˆ˜
        early_stopping_threshold=cfg["params"]["early_stopping_threshold"],  # ê°œì„ ì˜ ì„ê³„ê°’
        early_stopping_metric=cfg["params"]["early_stopping_metric"],  # í‰ê°€ ì§€í‘œ (ì—¬ê¸°ì„œëŠ” eval_loss ì‚¬ìš©)
        early_stopping_metric_minimize=cfg["params"]["early_stopping_metric_minimize"],  # í‰ê°€ ì§€í‘œë¥¼ ìµœì†Œí™”í•´ì•¼ í•˜ëŠ”ì§€ ì—¬ë¶€
    )

    # load dataset
    train_dataset = load_data(cfg["path"]["train_path"])
    dev_dataset = load_data(cfg["path"]["valid_path"])

    train_label = label_to_num(train_dataset["label"].values, cfg)
    dev_label = label_to_num(dev_dataset["label"].values, cfg)

    # tokenizing dataset
    tokenized_train = tokenized_dataset(train_dataset, tokenizer)
    tokenized_dev = tokenized_dataset(dev_dataset, tokenizer)

    # make dataset for pytorch
    RE_train_dataset = RE_Dataset(tokenized_train, train_label)
    RE_dev_dataset = RE_Dataset(tokenized_dev, dev_label)

    device = torch.device("cuda:0")

    print(device)
    prnt(info)
    # setting model hyperparameter
    model_config = AutoConfig.from_pretrained(MODEL_NAME)
    model_config.num_labels = 30

    # LSTM layerë¥¼ ì¶”ê°€í•œ Custom ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ë©´ ì•„ë˜ì˜ ì£¼ì„ ì²˜ë¦¬ëœ ë¶€ë¶„ì„ ì‚¬ìš©í•˜ì‹œë©´ ë©ë‹ˆë‹¤.
    # model = CustomRobertaForSequenceClassification.from_pretrained(MODEL_NAME, config=model_config)
    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=model_config)
    model.parameters
    model.to(device)

    training_args = TrainingArguments(
        output_dir=cfg["path"]["output_dir"],  #                                     output directory
        save_total_limit=cfg["params"]["save_total_limit"],  #                       number of total save model.
        save_steps=cfg["params"]["save_steps"],  #                                   model saving step.
        num_train_epochs=cfg["params"]["num_train_epochs"],  #                       total number of training epochs
        learning_rate=cfg["params"]["learning_rate"],  #                             learning_rate
        per_device_train_batch_size=cfg["params"]["per_device_train_batch_size"],  # batch size per device during training
        per_device_eval_batch_size=cfg["params"]["per_device_eval_batch_size"],  #   batch size for evaluation
        warmup_steps=cfg["params"]["warmup_steps"],  #                               number of warmup steps for learning rate scheduler
        weight_decay=cfg["params"]["weight_decay"],  #                               strength of weight decay
        logging_dir=cfg["path"]["logging_dir"],  #                                   directory for storing logs
        logging_steps=cfg["params"]["logging_steps"],  #                             log saving step.
        evaluation_strategy=cfg["params"]["evaluation_strategy"],  #                 evaluation strategy to adopt during training
        #                                                                            `no`: No evaluation during training.
        #                                                                            `steps`: Evaluate every `eval_steps`.
        #                                                                            `epoch`: Evaluate every end of epoch.
        eval_steps=cfg["params"]["eval_steps"],  #                                   evaluation step.
        load_best_model_at_end=cfg["params"]["load_best_model_at_end"],
        disable_tqdm=False,
        # save_strategy=cfg["params"]["evaluation_strategy"],
        # metric_for_best_model="micro f1 score",
    )

    get_focal = cfg["params"]["Get_Focal"]
    
    # Focal loss ì ìš© ì—¬ë¶€ ì„¤ì •
    if get_focal:
        custom_metrics = compute_metrics_focal
    else:
        custom_metrics = compute_metrics
        
    trainer = Trainer(
            model=model,  #                     the instantiated ğŸ¤— Transformers model to be trained
            args=training_args,  #              training arguments, defined above
            train_dataset=RE_train_dataset,  #  training dataset
            eval_dataset=RE_dev_dataset,  #     evaluation dataset
            compute_metrics=custom_metrics,  #  define metrics function
            callbacks=[early_stopping_callback], # ì–¼ë¦¬ ìŠ¤í†±í•‘ ì½œë°±ê³¼ WandB ì½œë°± ì¶”ê°€
        )

    # train model
    trainer.train()
    model.save_pretrained(cfg["path"]["MODEL_PATH"])

    # evaluate ë©”ì„œë“œë¥¼ í†µí•´ í‰ê°€ ìˆ˜í–‰
    evaluation_results = trainer.evaluate()
    # evaluation_resultsì—ëŠ” compute_metrics í•¨ìˆ˜ì—ì„œ ë°˜í™˜í•œ ë©”íŠ¸ë¦­ë“¤ì´ í¬í•¨ë¨

    # micro f1 score, auprc ì¶”ì¶œ
    micro_f1 = evaluation_results["eval_micro f1 score"]
    auprc = evaluation_results["eval_auprc"]
    acc = evaluation_results["eval_accuracy"]
    results = {"micro_f1": micro_f1, "auprc": auprc, "accuracy": acc}
    prnt(results)

    # difference.csv íŒŒì¼ ì¶œë ¥í•˜ê¸°
    pred = trainer.predict(RE_dev_dataset)
    preds = pred.predictions.argmax(-1)
    save_difference(preds, micro_f1, auprc)
    save_difference_png(micro_f1, auprc, cfg)

    # YAML íŒŒì¼ë¡œ ì €ì¥
    config_data = {"micro_f1": micro_f1, "auprc": auprc}
    with open(cfg["path"]["MODEL_PATH"] + "/metrics.yaml", "w") as yaml_file:
        yaml.dump(config_data, yaml_file)

    wandb.finish()


def main():
    train()


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=str, default="config/config.yaml", help="config file path")
    args = parser.parse_args()
    CONFIG_PATH = args.config

    try:
        cfg = load_config(CONFIG_PATH)  # yaml íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
    except:
        cfg = load_config("config/default_config.yaml")  # config.yaml íŒŒì¼ì´ ì—†ìœ¼ë©´ default íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°

    main()
